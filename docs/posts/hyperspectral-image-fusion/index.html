<!doctype html><html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://magamig.github.io/favicon.ico><link rel=stylesheet href=/css/style.min.css><title>Hyperspectral Image Fusion</title><script>!sessionStorage.getItem("_swa")&&document.referrer.indexOf(location.protocol+"//"+location.host)!==0&&fetch("https://counter.dev/track?"+new URLSearchParams({referrer:document.referrer,screen:screen.width+"x"+screen.height,user:"magamig",utcoffset:"1"})),sessionStorage.setItem("_swa","1")</script><meta name=google-site-verification content="oOj7T7YgBib7EQmE1PWwWTP1DmyTTUxA3yTfPPzVf5s"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.5/dist/katex.min.css integrity=sha384-L+Gq2Cso/Y2x8fX4wausgiZT8z0QPZz7OqPuz4YqAycQJyrJT9NRLpjFBD6zlOia crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.5/dist/katex.min.js integrity=sha384-z64WtjpyrKFsxox9eI4SI8eM9toXdoYeWb5Qh+8PO+eG54Bv9BZqf9xNhlcLf/sA crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.5/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})'></script></head><body><div class=container><header id=banner><h2><a href=https://magamig.github.io/>Miguel Magalhães</a></h2><nav><ul><li>»
<a href=/posts/ title=Posts>Posts</a></li><li><a href=/map/ title=Map>Map</a></li><li><a href=/beers/ title=Beers>Beers</a></li><li><a href=/about/ title=About>About</a></li></ul></nav></header><main id=content><article><header id=post-header><h1>Hyperspectral Image Fusion</h1><time>June 24, 2023
· Ireland</time></header><p>Hyperspectral imaging (HSI) collects several images (bands) over a wide and continuous wavelength range. This forms a hyperspectral (HS) cube formed by 3 dimensions &mdash; 2 for the spatial position and 1 for the spectral coordinate ($x,y,\lambda$). <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><p><img src=/image/spectralcube.png alt></p><p>HSI has proven its usefulness with its rich spectral information, but <strong>lacks acutely in terms of spatial resolution</strong>. This is caused by <strong>hardware limitations</strong> &mdash; a long exposure is necessary to collect enough photons while maintaining a good signal-to-noise ratio, leading to low spatial resolutions. <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup></p><p>The <strong>lack of spatial resolution</strong> hinders the development of further HSI applications and diminishes the accuracy of the already existing ones. Nevertheless, the hardware limitations can be overcome with <strong>software-based approaches</strong> to improve the resolution of hyperspectral images.</p><p>These approaches are named Hyperspectral Super Resolution methods (HSSR). These methods which improve the spatial resolution of hyperspectral images: (1) <strong>improves the applicability</strong> of this technology across all the pre-existing applications, and (2) allows for other <strong>novel usages</strong> that would otherwise not have been possible with the available low-resolution HS images.</p><p>This text is focused on Hyperspectral Image Fusion (HIF) which is a sub-tupe of HSSR. The goal of HIF is to obtain an accurate super-resolution hyperspectral image from two input images: a low-spatial high-spectral resolution image and a high-spatial low-spectral resolution image.</p><p><img src=/image/hif_input_output.png alt></p><p>The image above can be described as follows: a HS cube contains the spectrum of light for each pixel and is formed by two dimensions that represent the spatial position ($x,y$), and a third that is the spectral coordinate ($\lambda$). Therefore, a cube $\mathbf{C}$ can be mathematically described as $\mathbf{C} \in \mathbb{R}^{x \times y \times \lambda}$.</p><p>Based on this convention, the inputs and corresponding output of the system can be formally summarized as follows. Let $\mathbf{HS} \in \mathbb{R}^{w \times h \times \Lambda}$ and $\mathbf{RGB} \in \mathbb{R}^{W \times H \times \lambda}$ be the two input images. The variables $w$, $h$ and $\lambda$ denote the width, the height and the spectral dimension respectively; with the same capital letters corresponding to the same variable but with high value, such that $W \gg w$, $H \gg h$ and $\Lambda \gg \lambda$. Additionally, $\lambda=3$ since the RGB image has three color channels RGB. From these inputs, we obtain the super-resolution hyperspectral image $\mathbf{SR} \in \mathbb{R}^{W \times H \times \Lambda}$ through</p><center>$\boldsymbol{\Psi}: \mathbb{R}^{w \times h \times \Lambda} \times \mathbb{R}^{W \times H \times 3} \rightarrow \mathbb{R}^{W \times H \times \Lambda}$<p>$\mathbf{SR} = \Psi(\mathbf{HS},\mathbf{RGB})$</center></p><h3 id=walds-protocol>Wald’s Protocol</h3><p>In 1997, Wald et. al. <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> proposed what would be named Wald’s Protocol, a paradigm for quality assessment of fused images, which can be described as follows:</p><ol><li>From the HS reference image, we produce two synthetic images that are going to be the input to the HIF method: (1) a low spatial resolution HS image, and (2) a high spatial resolution RGB image. To synthesize the low-spatial resolution hyperspectral image, the high-spatial resolution hyperspectral ground truth (GT) image is blurred and downsampled by a pre-defined scaling factor to a smaller spatial resolution; and to synthesize the RGB image we typically simulate a spectral response of an RGB camera over the GT image.</li><li>Those two images serve as input to the HIF method that we are testing, which in turn produces a super-resolution (SR) HS image.</li><li>The output SR HS image is then compared against the hyperspectral GT reference image. This is used to compute quality metrics and perform a visual analysis of the results.</li></ol><p><img src=/image/walds.jpg alt></p><p>To fully evaluate a HIF method it is necessary to have <strong>full-reference quality assessment metrics</strong> which ensure an objective comparison of the resolution enhancement process. These metrics can assess quality in the <strong>spectral domain</strong> (SAM and SID), in the <strong>spatial domain</strong> (SCC), or assess the <strong>global image quality</strong> (Total Error, RMSE, RASE, ERGAS, PSNR, SSIM, MS-SSIM, PSNR-B, UQI, VIF and Q2<sup>n</sup>).</p><hr><p>If you are interested to learn more about the topic, please refer to my thesis titled <strong>&ldquo;Hyperspectral Image Fusion: A Comprehensive review&rdquo;</strong>, which contains a <strong>comprehensive review of the state-of-the-art of HIF</strong>. It includes a compilation of numerous HIF methods, tested with several image databases while measuring many quality metrics all at once, and in a generalized, fair, and extendable testing protocol.</p><div class=info><p><strong>Hyperspectral Image Fusion: A Comprehensive review</strong></p><ul><li><a href=https://github.com/magamig/hif-benchmarking/blob/main/thesis.pdf target=_blank>Thesis (PDF)</a></li><li><a href=https://github.com/magamig/hif-benchmarking/blob/main/slides.pdf target=_blank>Slides (PDF)</a></li><li><a href=https://github.com/magamig/hif-benchmarking target=_blank>Code Repository</a></li></ul></div><hr><p>If you use any part of this work, please use the following citation:</p><p><i>Magalhães, Miguel. “Hyperspectral Image Fusion: A Comprehensive Review”. Master’s Programme in Imaging and Light in Extended Reality (IMLEX). MSc. thesis. KU Leuven, 2022.</i></p><p><code>@mastersthesis{hif_review_2022,
title={Hyperspectral Image Fusion: A Comprehensive Review},
author={Miguel Magalhães},
year={2022},
school={KU Leuven},
note={Master’s Programme in Imaging and Light in Extended Reality (IMLEX)}
}</code></p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Manolakis, D., Marden, D., & Shaw, G. A. (2003). Hyperspectral image processing for automatic target detection applications. Lincoln laboratory journal, 14(1), 79-116.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Akhtar, N., Shafait, F., & Mian, A. (2014). Sparse spatio-spectral representation for hyperspectral image super-resolution. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII 13 (pp. 63-78). Springer International Publishing.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Wald, L., Ranchin, T., & Mangolini, M. (1997). Fusion of satellite images of different spatial resolutions: Assessing the quality of resulting images. Photogrammetric engineering and remote sensing, 63(6), 691-699.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></article></main><footer id=footer></footer><div></body></html>